{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Импортируем все необходимые библиотеки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.models as models\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.26.4\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "print(np.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Трансформации для изображений\n",
    "transform = transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels=1), #преобразование в оттенки серого\n",
    "    transforms.Resize((14, 14)), #14х14 пикселей\n",
    "    transforms.ToTensor(), #перевод в тензоры\n",
    "    transforms.Normalize((0.5,), (0.5,)) #нормализовка изображения\n",
    "])\n",
    "\n",
    "# Загрузчики данных\n",
    "train_data = datasets.ImageFolder(root='final_symbols_split_ttv/train', transform=transform)\n",
    "val_data = datasets.ImageFolder(root='final_symbols_split_ttv/val', transform=transform)\n",
    "test_data = datasets.ImageFolder(root='final_symbols_split_ttv/test', transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=32, shuffle=True, drop_last=True) # Создание загрузчика данных с batch size 32, перемешиванием данных и отбрасыванием последнего неполного батча\n",
    "val_loader = DataLoader(val_data, batch_size=32, shuffle=False, drop_last=True)\n",
    "test_loader = DataLoader(test_data, batch_size=32, shuffle=False, drop_last=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Изначальная модель - была отменена потому, что слишком долго выполнялось обучение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class SimpleCNN(nn.Module):\n",
    "#     def __init__(self, num_classes):\n",
    "#         super(SimpleCNN, self).__init__()\n",
    "#         self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1)\n",
    "#         self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "#         self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0) #Макспулинг слой (уменьшает размер изображения вдвое).\n",
    "#         self.fc1 = nn.Linear(64 * 7 * 7, 128)\n",
    "#         self.fc2 = nn.Linear(128, num_classes)  # Измените на num_classes\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = self.pool(F.relu(self.conv1(x)))\n",
    "#         x = self.pool(F.relu(self.conv2(x)))\n",
    "#         x = x.view(-1, 64 * 7 * 7)\n",
    "#         x = F.relu(self.fc1(x))\n",
    "#         x = self.fc2(x)\n",
    "#         return x\n",
    "\n",
    "# # Количество классов\n",
    "# num_classes = len(train_data.classes)\n",
    "# model = SimpleCNN(num_classes=num_classes)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Использование предварительно обученной модели ResNet18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Количество классов: 17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "f:\\pet2\\pythonProject4\\.venv\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "f:\\pet2\\pythonProject4\\.venv\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "num_classes = len(train_data.classes)\n",
    "print(f'Количество классов: {num_classes}')\n",
    "\n",
    "# Использование предварительно обученной модели ResNet18\n",
    "model = models.resnet18(pretrained=True) # Загрузка предобученной модели ResNet18 с предобученными весами\n",
    "model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)  # Изменение входного слоя для одноканальных изображений\n",
    "model.fc = nn.Linear(model.fc.in_features, num_classes) # Изменение последнего полностью связанного слоя для классификации на num_classes классов\n",
    "model = model.to(device) # Перенос модели на устройство"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сверточные слои используются для извлечения признаков из изображений. Они применяют фильтры (ядра свертки) к входному изображению для создания карт признаков. В первом слое используется 32 фильтра, во втором - 64 фильтра. Размер ядра (kernel size) выбран равным 3x3, что является стандартным выбором для сверточных сетей. stride=1 означает, что фильтр перемещается на один пиксель за раз, а padding=1 добавляет один пиксель со всех сторон изображения для сохранения его размеров после свертки.\n",
    "\n",
    "Функция активации ReLU (Rectified Linear Unit) используется для введения нелинейности в модель. Она заменяет все отрицательные значения нулями, что помогает модели обучаться сложным зависимостям в данных.\n",
    "\n",
    "Макспулинг слои уменьшают размер карт признаков, оставляя только наиболее значимые признаки. В данном случае используется слой пулинга с ядром 2x2 и шагом 2, что уменьшает размер входных данных в два раза.\n",
    "\n",
    "Полносвязные слои используются для окончательной классификации признаков, извлеченных сверточными слоями. В первом полносвязном слое используются 128 нейронов, а во втором - количество нейронов равно количеству классов (num_classes). Размер входа в первый полносвязный слой равен 64 * 7 * 7, что соответствует количеству карт признаков после второго сверточного слоя и двух слоев пулинга."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss() # Определение функции потерь (кросс-энтропия)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01) # Определение оптимизатора Adam с начальной скоростью обучения 0.01\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1) # Определение планировщика изменения скорости обучения: каждые 3 эпохи уменьшать скорость обучения в 10 раз"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/4: 100%|██████████| 6586/6586 [1:38:38<00:00,  1.11batch/s, loss=0.529]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/4], Loss: 0.5285\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 168/168 [00:46<00:00,  3.59batch/s, accuracy=88.9, val_loss=0.358]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.3576, Accuracy: 88.91%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/4: 100%|██████████| 6586/6586 [43:42<00:00,  2.51batch/s, loss=0.124]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/4], Loss: 0.1239\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 168/168 [00:09<00:00, 18.20batch/s, accuracy=93.1, val_loss=0.232] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.2323, Accuracy: 93.14%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/4: 100%|██████████| 6586/6586 [39:05<00:00,  2.81batch/s, loss=0.0879]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/4], Loss: 0.0879\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 168/168 [00:09<00:00, 18.24batch/s, accuracy=91.9, val_loss=0.262]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.2622, Accuracy: 91.89%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/4: 100%|██████████| 6586/6586 [39:48<00:00,  2.76batch/s, loss=0.0383]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/4], Loss: 0.0383\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 168/168 [00:08<00:00, 18.76batch/s, accuracy=97.3, val_loss=0.0949]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.0949, Accuracy: 97.27%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 4 # Количество раз, которое мы будем проходиться по папке train и val\n",
    "for epoch in range(num_epochs):\n",
    "    model.train() # Перевод модели в режим обучения (активируются слои, такие как Dropout и BatchNorm)\n",
    "    running_loss = 0.0 # Начальные потери(неудачи)\n",
    "    with tqdm(train_loader, unit=\"batch\") as tepoch:  # Используем tqdm для отображения прогресса\n",
    "        for images, labels in tepoch: # \n",
    "            tepoch.set_description(f\"Epoch {epoch + 1}/{num_epochs}\") # Установка описания для текущей эпохи в tqdm\n",
    "            optimizer.zero_grad() # Обнуление градиентов (частных производных), указывающих направления наибольшего изменения функции потерь\n",
    "            outputs = model(images) # Прогонка изображений через модель для получения выходных данных\n",
    "            loss = criterion(outputs, labels) # Вычисление функции потерь (ошибки) между выходными данными модели и истинными метками\n",
    "            loss.backward() # Обратное распространение ошибки для вычисления градиентов\n",
    "            optimizer.step() # Обновление параметров модели на основе вычисленных градиентов\n",
    "            running_loss += loss.item() # Суммирование потерь для текущего батча\n",
    "            tepoch.set_postfix(loss=running_loss / len(train_loader)) # Обновление отображаемого значения потерь в tqdm\n",
    "    \n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader):.4f}')\n",
    "\n",
    "    model.eval() # Перевод модели в режим оценки (деактивируются слои Dropout и BatchNorm)\n",
    "    val_loss = 0.0 # Инициализация переменной для хранения суммарной потери на валидационных данных\n",
    "    correct = 0 # Инициализация переменной для подсчета количества правильных предсказаний\n",
    "    total = 0 # Инициализация переменной для подсчета общего количества примеров\n",
    "    with torch.no_grad(): # Отключение вычисления градиентов (ускоряет и снижает потребление памяти)\n",
    "        with tqdm(val_loader, unit=\"batch\") as vepoch:  # Используем tqdm для отображения прогресса\n",
    "            for images, labels in vepoch:\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "                vepoch.set_postfix(val_loss=val_loss / len(val_loader), accuracy=100 * correct / total)\n",
    "    \n",
    "    print(f'Validation Loss: {val_loss/len(val_loader):.4f}, Accuracy: {100 * correct / total:.2f}%')\n",
    "\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Сохранение модели\n",
    "torch.save(model.state_dict(), 'model.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для проверки запускай код ниже"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:37: SyntaxWarning: invalid escape sequence '\\p'\n",
      "<>:37: SyntaxWarning: invalid escape sequence '\\p'\n",
      "C:\\Users\\uiop1\\AppData\\Local\\Temp\\ipykernel_10764\\3667394140.py:37: SyntaxWarning: invalid escape sequence '\\p'\n",
      "  model.load_state_dict(torch.load('F:\\pet2\\pythonProject4\\.src\\model.pth'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['=', 'add', 'divide', 'eight', 'five', 'four', 'gt', 'lt', 'multiply', 'nine', 'one', 'seven', 'six', 'subtract', 'three', 'two', 'zero']\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.models as models\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Трансформации для изображений\n",
    "transform = transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels=1), #преобразование в оттенки серого\n",
    "    transforms.Resize((14, 14)), #14х14 пикселей\n",
    "    transforms.ToTensor(), #перевод в тензоры\n",
    "    transforms.Normalize((0.5,), (0.5,)) #нормализовка изображения\n",
    "])\n",
    "\n",
    "# Загрузка предобученной модели ResNet18\n",
    "model = models.resnet18(pretrained=False)\n",
    "num_ftrs = model.fc.in_features\n",
    "\n",
    "# Изменение первого свёрточного слоя для работы с одноканальными изображениями\n",
    "model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "\n",
    "# Изменение последнего полностью связанного слоя для 17 классов\n",
    "model.fc = nn.Linear(num_ftrs, 17)\n",
    "\n",
    "# Загрузка сохранённых весов\n",
    "model.load_state_dict(torch.load('F:\\pet2\\pythonProject4\\.src\\model.pth'))\n",
    "model.eval()  # Перевод модели в режим оценки\n",
    "\n",
    "train_dataset = datasets.ImageFolder('final_symbols_split_ttv/train', transform=transform)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "class_labels = train_dataset.classes\n",
    "print(class_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Предсказанный класс: 4, Метка класса: five\n"
     ]
    }
   ],
   "source": [
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "# Трансформации для изображений\n",
    "transform = transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels=1),\n",
    "    transforms.Resize((14, 14)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "# Функция для предсказания\n",
    "def predict_image(image_path, model, transform, class_labels):\n",
    "    image = Image.open(image_path)\n",
    "    image = transform(image).unsqueeze(0)  # Добавить размер batch\n",
    "    outputs = model(image)\n",
    "    _, predicted = torch.max(outputs, 1)\n",
    "    predicted_class = predicted.item()\n",
    "    predicted_label = class_labels[predicted_class]\n",
    "    return predicted_class, predicted_label\n",
    "\n",
    "\n",
    "# Пример использования\n",
    "image_path = 'output.png'\n",
    "predicted_class, predicted_label = predict_image(image_path, model, transform, class_labels)\n",
    "print(f'Предсказанный класс: {predicted_class}, Метка класса: {predicted_label}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ПЕРЕОБУЧИТЬ МОДЕЛЬ НА RESIZE 28 28, ТАКЖЕ УВЕЛИЧИТЬ КОЛИЧЕСТВО ЭПОХ И ПОПЫТАТЬСЯ УВЕЛИЧИТЬ СЛОЕВ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:37: SyntaxWarning: invalid escape sequence '\\p'\n",
      "<>:37: SyntaxWarning: invalid escape sequence '\\p'\n",
      "C:\\Users\\uiop1\\AppData\\Local\\Temp\\ipykernel_2140\\1163673216.py:37: SyntaxWarning: invalid escape sequence '\\p'\n",
      "  model.load_state_dict(torch.load('F:\\pet2\\pythonProject4\\.src\\model.pth'))\n",
      "f:\\pet2\\pythonProject4\\.venv\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "f:\\pet2\\pythonProject4\\.venv\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['=', 'add', 'divide', 'eight', 'five', 'four', 'gt', 'lt', 'multiply', 'nine', 'one', 'seven', 'six', 'subtract', 'three', 'two', 'zero']\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.models as models\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Инициализация списков для хранения значений потерь и точности\n",
    "train_losses = []\n",
    "test_accuracies = []\n",
    "\n",
    "# Трансформации для изображений\n",
    "transform = transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels=1), #преобразование в оттенки серого\n",
    "    transforms.Resize((28, 14)), #14х14 пикселей\n",
    "    transforms.ToTensor(), #перевод в тензоры\n",
    "    transforms.Normalize((0.5,), (0.5,)) #нормализовка изображения\n",
    "])\n",
    "\n",
    "# Загрузка предобученной модели ResNet18\n",
    "model = models.resnet18(pretrained=False)\n",
    "num_ftrs = model.fc.in_features\n",
    "\n",
    "# Изменение первого свёрточного слоя для работы с одноканальными изображениями\n",
    "model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "\n",
    "# Изменение последнего полностью связанного слоя для 17 классов\n",
    "model.fc = nn.Linear(num_ftrs, 17)\n",
    "\n",
    "# Загрузка сохранённых весов\n",
    "model.load_state_dict(torch.load('F:\\pet2\\pythonProject4\\.src\\model.pth'))\n",
    "model.eval()  # Перевод модели в режим оценки\n",
    "\n",
    "train_dataset = datasets.ImageFolder('final_symbols_split_ttv/train', transform=transform)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "class_labels = train_dataset.classes\n",
    "print(class_labels)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:12: SyntaxWarning: invalid escape sequence '\\p'\n",
      "<>:12: SyntaxWarning: invalid escape sequence '\\p'\n",
      "C:\\Users\\uiop1\\AppData\\Local\\Temp\\ipykernel_2140\\2365840718.py:12: SyntaxWarning: invalid escape sequence '\\p'\n",
      "  model.load_state_dict(torch.load('F:\\pet2\\pythonProject4\\.src\\model.pth'))\n",
      "f:\\pet2\\pythonProject4\\.venv\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "f:\\pet2\\pythonProject4\\.venv\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n",
      "Epoch 1/4:   0%|          | 22/6587 [00:36<3:01:19,  1.66s/batch, loss=0.0166]\n",
      "C:\\Users\\uiop1\\AppData\\Local\\Temp\\ipykernel_2140\\2365840718.py:12: SyntaxWarning: invalid escape sequence '\\p'\n",
      "  model.load_state_dict(torch.load('F:\\pet2\\pythonProject4\\.src\\model.pth'))\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 30\u001b[0m\n\u001b[0;32m     28\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(images) \u001b[38;5;66;03m# Прогонка изображений через модель для получения выходных данных\u001b[39;00m\n\u001b[0;32m     29\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels) \u001b[38;5;66;03m# Вычисление функции потерь (ошибки) между выходными данными модели и истинными метками\u001b[39;00m\n\u001b[1;32m---> 30\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# Обратное распространение ошибки для вычисления градиентов\u001b[39;00m\n\u001b[0;32m     31\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep() \u001b[38;5;66;03m# Обновление параметров модели на основе вычисленных градиентов\u001b[39;00m\n\u001b[0;32m     32\u001b[0m running_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;66;03m# Суммирование потерь для текущего батча\u001b[39;00m\n",
      "File \u001b[1;32mf:\\pet2\\pythonProject4\\.venv\\Lib\\site-packages\\torch\\_tensor.py:525\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    517\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    518\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    523\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    524\u001b[0m     )\n\u001b[1;32m--> 525\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    526\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    527\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mf:\\pet2\\pythonProject4\\.venv\\Lib\\site-packages\\torch\\autograd\\__init__.py:267\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    262\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    264\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    265\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    266\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 267\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    275\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mf:\\pet2\\pythonProject4\\.venv\\Lib\\site-packages\\torch\\autograd\\graph.py:744\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    742\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    743\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 744\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    745\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    746\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    747\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    748\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Загрузка предобученной модели ResNet18\n",
    "model = models.resnet18(pretrained=False)\n",
    "num_ftrs = model.fc.in_features\n",
    "\n",
    "# Изменение первого свёрточного слоя для работы с одноканальными изображениями\n",
    "model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "\n",
    "# Изменение последнего полностью связанного слоя для 17 классов\n",
    "model.fc = nn.Linear(num_ftrs, 17)\n",
    "\n",
    "# Загрузка сохранённых весов\n",
    "model.load_state_dict(torch.load('F:\\pet2\\pythonProject4\\.src\\model.pth'))\n",
    "model.eval()  # Перевод модели в режим оценки\n",
    "\n",
    "criterion = nn.CrossEntropyLoss() # Определение функции потерь (кросс-энтропия)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01) # Определение оптимизатора Adam с начальной скоростью обучения 0.01\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1) # Определение планировщика изменения скорости обучения: каждые 3 эпохи уменьшать скорость обучения в 10 раз\n",
    "\n",
    "\n",
    "num_epochs = 4 # Количество раз, которое мы будем проходиться по папке train и val\n",
    "for epoch in range(num_epochs):\n",
    "    model.train() # Перевод модели в режим обучения (активируются слои, такие как Dropout и BatchNorm)\n",
    "    running_loss = 0.0 # Начальные потери(неудачи)\n",
    "    with tqdm(train_loader, unit=\"batch\") as tepoch:  # Используем tqdm для отображения прогресса\n",
    "        for images, labels in tepoch: # \n",
    "            tepoch.set_description(f\"Epoch {epoch + 1}/{num_epochs}\") # Установка описания для текущей эпохи в tqdm\n",
    "            optimizer.zero_grad() # Обнуление градиентов (частных производных), указывающих направления наибольшего изменения функции потерь\n",
    "            outputs = model(images) # Прогонка изображений через модель для получения выходных данных\n",
    "            loss = criterion(outputs, labels) # Вычисление функции потерь (ошибки) между выходными данными модели и истинными метками\n",
    "            loss.backward() # Обратное распространение ошибки для вычисления градиентов\n",
    "            optimizer.step() # Обновление параметров модели на основе вычисленных градиентов\n",
    "            running_loss += loss.item() # Суммирование потерь для текущего батча\n",
    "            tepoch.set_postfix(loss=running_loss / len(train_loader)) # Обновление отображаемого значения потерь в tqdm\n",
    "    \n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader):.4f}')\n",
    "\n",
    "    model.eval() # Перевод модели в режим оценки (деактивируются слои Dropout и BatchNorm)\n",
    "    val_loss = 0.0 # Инициализация переменной для хранения суммарной потери на валидационных данных\n",
    "    correct = 0 # Инициализация переменной для подсчета количества правильных предсказаний\n",
    "    total = 0 # Инициализация переменной для подсчета общего количества примеров\n",
    "    with torch.no_grad(): # Отключение вычисления градиентов (ускоряет и снижает потребление памяти)\n",
    "        with tqdm(val_loader, unit=\"batch\") as vepoch:  # Используем tqdm для отображения прогресса\n",
    "            for images, labels in vepoch:\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "                vepoch.set_postfix(val_loss=val_loss / len(val_loader), accuracy=100 * correct / total)\n",
    "    \n",
    "    print(f'Validation Loss: {val_loss/len(val_loader):.4f}, Accuracy: {100 * correct / total:.2f}%')\n",
    "        # Построение графиков\n",
    "    fig, ax = plt.subplots(2, 1, figsize=(10, 8))\n",
    "\n",
    "    # График потерь\n",
    "    ax[0].plot(range(1, num_epochs+1), train_losses, label='Train Loss')\n",
    "    ax[0].set_xlabel('Epochs')\n",
    "    ax[0].set_ylabel('Loss')\n",
    "    ax[0].set_title('Training Loss')\n",
    "    ax[0].legend()\n",
    "\n",
    "    # График точности\n",
    "    ax[1].plot(range(1, num_epochs+1), test_accuracies, label='Test Accuracy')\n",
    "    ax[1].set_xlabel('Epochs')\n",
    "    ax[1].set_ylabel('Accuracy')\n",
    "    ax[1].set_title('Test Accuracy')\n",
    "    ax[1].legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Сохранение модели\n",
    "torch.save(model.state_dict(), 'model2.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
